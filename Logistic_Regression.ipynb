{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hdt9heGqhos3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6832dcb5-f9ad-4f57-c793-693dfd92befa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.1\n"
          ]
        }
      ],
      "source": [
        "# !pip uninstall tensorflow\n",
        "# !pip install tensorflow==1.13.1\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# import tensorflow as tf\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "C9Myas9lic1N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "metadata": {
        "id": "ZcPItMIDjjfY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape(60000, 28 * 28) / 255\n",
        "x_test = x_test.reshape(10000, 28 * 28) / 255\n",
        "with tf.Session() as sesh:\n",
        "    y_train = sesh.run(tf.one_hot(y_train, 10))\n",
        "    y_test = sesh.run(tf.one_hot(y_test, 10))"
      ],
      "metadata": {
        "id": "GuOLVaiFjsRt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops import control_flow_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import state_ops\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.training import optimizer\n",
        "#from tensorflow.python.eager import context\n",
        "from tensorflow.python.ops import resource_variable_ops\n",
        "from tensorflow.python.ops import variable_scope\n",
        "from tensorflow.python.training import training_ops\n",
        "class AdamOptimizer(optimizer.Optimizer):\n",
        "    \"\"\"Optimizer that implements the Adam optimizer.\n",
        "    See [Kingma et. al., 2014](http://arxiv.org/abs/1412.6980)\n",
        "    ([pdf](http://arxiv.org/pdf/1412.6980.pdf)).\n",
        "    @@__init__\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, use_locking=False, epsilon=1e-8, name=\"Adam\"):\n",
        "    # def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.699, use_locking=False, epsilon=1e-8, name=\"Adam\"):\n",
        "        super(AdamOptimizer, self).__init__(use_locking, name)\n",
        "        self._lr = learning_rate\n",
        "        self._beta1 = beta1\n",
        "        self._beta2 = beta2\n",
        "\n",
        "        # Tensor versions of the constructor arguments, created in _prepare().\n",
        "        self._lr_t = None\n",
        "        self._beta1_t = None\n",
        "        self._beta2_t = None\n",
        "        self._beta1_power = None\n",
        "        self._beta2_power = None\n",
        "\n",
        "    def _prepare(self):\n",
        "        self._lr_t = ops.convert_to_tensor(self._lr, name=\"learning_rate\")\n",
        "        self._beta1_t = ops.convert_to_tensor(self._beta1, name=\"beta1\")\n",
        "        self._beta2_t = ops.convert_to_tensor(self._beta2, name=\"beta2\")\n",
        "\n",
        "    def _create_slots(self, var_list):\n",
        "        # Create slots for the first and second moments.\n",
        "        first_var = min(var_list, key=lambda x: x.name)\n",
        "        with ops.colocate_with(first_var):\n",
        "            self._beta1_power = variable_scope.variable(self._beta1, name=\"beta1_power\", trainable=False)\n",
        "            self._beta2_power = variable_scope.variable(self._beta2, name=\"beta2_power\", trainable=False)\n",
        "        # Create slots for the first and second moments.\n",
        "        for v in var_list:\n",
        "            self._zeros_slot(v, \"m1\", self._name)\n",
        "            self._zeros_slot(v, \"v1\", self._name)\n",
        "\n",
        "    def _apply_dense(self, grad, var):\n",
        "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
        "        beta1_power = math_ops.cast(self._beta1_power, var.dtype.base_dtype)\n",
        "        beta2_power = math_ops.cast(self._beta2_power, var.dtype.base_dtype)\n",
        "        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n",
        "        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n",
        "        eps = 1e-8 \n",
        "        v = self.get_slot(var, \"v1\")\n",
        "        # updating v_rms beta_2_t\n",
        "        v_t = v.assign(beta2_t * v + (1. - beta2_t) * grad**2)\n",
        "        m = self.get_slot(var, \"m1\")\n",
        "       # updating momentum beta_1_t.\n",
        "        m_t = m.assign(beta1_t * m + (1. - beta1_t) * grad)\n",
        "\n",
        "        # alpha_t is basically bias correction\n",
        "\n",
        "        # with bias correction\n",
        "        alpha_t =  tf.sqrt(1 - beta2_power) / (1 - beta1_power)\n",
        "        \n",
        "        # without bias correction\n",
        "        # alpha_t =  1 # uncomment if no bias correction \n",
        "         \n",
        "        g_t =  (m_t*alpha_t) / (tf.sqrt(v_t) + eps)\n",
        "        var_update = state_ops.assign_sub(var, lr_t * g_t)\n",
        "        return control_flow_ops.group(*[var_update, v_t, m_t])\n",
        "\n",
        "    def _apply_sparse(self, grad, var):\n",
        "        raise NotImplementedError(\"Sparse gradient updates are not supported.\")"
      ],
      "metadata": {
        "id": "kIfNcAQZkoE_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_options = ['SGDNesterov', 'Adagrad', 'RMSProp', 'AdaDelta', 'Adam']\n",
        "dropout_options = [False]"
      ],
      "metadata": {
        "id": "XZCM-Ul3lBYg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimizer_fn(optimizer, lr, loss, name='Optimizer'):\n",
        "    with tf.variable_scope(name):\n",
        "        global_step = tf.Variable(1, dtype=tf.float32, trainable=False)\n",
        "        cur_lr = lr / tf.math.sqrt(x=global_step)\n",
        "\n",
        "        if optimizer == 'SGDNesterov':\n",
        "            return tf.train.MomentumOptimizer(learning_rate=cur_lr,\n",
        "                                              momentum=0.99,\n",
        "                                              name='SGDNesterov',\n",
        "                                              use_nesterov=True).minimize(loss, global_step=global_step), cur_lr\n",
        "        elif optimizer == 'Adagrad':\n",
        "            return tf.train.AdagradOptimizer(learning_rate=cur_lr).minimize(loss, global_step=global_step), cur_lr\n",
        "        elif optimizer == 'RMSProp':\n",
        "            return tf.train.RMSPropOptimizer(learning_rate=cur_lr).minimize(loss, global_step=global_step), cur_lr\n",
        "        elif optimizer == 'AdaDelta':\n",
        "            return tf.train.AdadeltaOptimizer(learning_rate=cur_lr).minimize(loss, global_step=global_step), cur_lr\n",
        "        elif optimizer == 'Adam':\n",
        "            return AdamOptimizer(learning_rate=cur_lr).minimize(loss, global_step=global_step), cur_lr\n",
        "        else:\n",
        "            raise NotImplementedError(\" [*] Optimizer is not included in list!\")\n",
        "\n",
        "learning_rate = 0.001\n",
        "epochs = 20\n",
        "batch_size = 100\n",
        "batches = int(x_train.shape[0] / batch_size)\n",
        "for optimizer in optimizer_options:\n",
        "  print('\\nOptimizer: {}\\n'.format(optimizer))\n",
        "  X = tf.placeholder(tf.float32, [None, 784])\n",
        "  Y = tf.placeholder(tf.float32, [None, 10])\n",
        "  W = tf.Variable(0.1 * np.random.randn(784, 10).astype(np.float32))\n",
        "  B = tf.Variable(0.1 * np.random.randn(10).astype(np.float32))\n",
        "  Y_predicted = tf.nn.softmax(tf.add(tf.matmul(X, W), B))\n",
        "  loss = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(Y_predicted), axis=1))\n",
        "  optimizer = optimizer_fn(optimizer,learning_rate,loss)\n",
        "  with tf.Session () as sess:\n",
        "    sess.run ( tf.global_variables_initializer ( ) )\n",
        "    for epoch in range(epochs):\n",
        "      for i in range(batches):\n",
        "        offset = i * epoch\n",
        "        x = x_train[offset: offset + batch_size]\n",
        "        y = y_train[offset: offset + batch_size]\n",
        "        sess.run(optimizer, feed_dict={X: x, Y:y})\n",
        "        c = sess.run(loss, feed_dict={X:x, Y:y})\n",
        "      print(f'epoch:{epoch:2d} cost={c:.4f}')\n",
        "    correct_pred = tf.equal(tf.argmax(Y_predicted, 1), tf.argmax(Y, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "    acc = accuracy.eval({X: x_test, Y: y_test})\n",
        "    print(f'Accuracy: {acc * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT_Yr1C0okzW",
        "outputId": "f1738f8e-0ef8-4ee7-9cde-d8365409e369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimizer: SGDNesterov\n",
            "\n",
            "epoch: 0 cost=0.5167\n",
            "epoch: 1 cost=1.1453\n",
            "epoch: 2 cost=0.8369\n",
            "epoch: 3 cost=0.6429\n",
            "epoch: 4 cost=0.7038\n",
            "epoch: 5 cost=0.8195\n",
            "epoch: 6 cost=0.5665\n",
            "epoch: 7 cost=0.5438\n",
            "epoch: 8 cost=0.7227\n",
            "epoch: 9 cost=0.4483\n",
            "epoch:10 cost=0.3903\n",
            "epoch:11 cost=0.4220\n",
            "epoch:12 cost=0.8112\n",
            "epoch:13 cost=0.6350\n",
            "epoch:14 cost=0.6367\n",
            "epoch:15 cost=0.4170\n",
            "epoch:16 cost=0.5601\n",
            "epoch:17 cost=0.6361\n",
            "epoch:18 cost=0.3964\n",
            "epoch:19 cost=0.4092\n",
            "Accuracy: 86.80%\n",
            "\n",
            "Optimizer: Adagrad\n",
            "\n",
            "epoch: 0 cost=2.3968\n",
            "epoch: 1 cost=2.5340\n",
            "epoch: 2 cost=2.4367\n",
            "epoch: 3 cost=2.3738\n",
            "epoch: 4 cost=2.3383\n",
            "epoch: 5 cost=2.2815\n",
            "epoch: 6 cost=2.4239\n",
            "epoch: 7 cost=2.2732\n",
            "epoch: 8 cost=2.4704\n",
            "epoch: 9 cost=2.3921\n",
            "epoch:10 cost=2.2625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dropout in LR\n",
        "- Demos above Adam class\n",
        "- Switch case in optimizer function\n",
        "- Neural\n",
        "- CNN (CIPHAR dataset required)"
      ],
      "metadata": {
        "id": "ze_nPsgX4JyH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nfbK3KzN5YJt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}