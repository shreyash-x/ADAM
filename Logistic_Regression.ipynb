{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdt9heGqhos3",
        "outputId": "6832dcb5-f9ad-4f57-c793-693dfd92befa"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall tensorflow\n",
        "# !pip install tensorflow==1.13.1\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9Myas9lic1N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import logging\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "# import tensorflow as tf\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcPItMIDjjfY"
      },
      "outputs": [],
      "source": [
        "# MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuOLVaiFjsRt"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.reshape(60000, 28 * 28) / 255\n",
        "x_test = x_test.reshape(10000, 28 * 28) / 255\n",
        "with tf.Session() as sesh:\n",
        "    y_train = sesh.run(tf.one_hot(y_train, 10))\n",
        "    y_test = sesh.run(tf.one_hot(y_test, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIfNcAQZkoE_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.ops import control_flow_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import state_ops\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.training import optimizer\n",
        "#from tensorflow.python.eager import context\n",
        "from tensorflow.python.ops import resource_variable_ops\n",
        "from tensorflow.python.ops import variable_scope\n",
        "from tensorflow.python.training import training_ops\n",
        "class AdamOptimizer(optimizer.Optimizer):\n",
        "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, use_locking=False, epsilon=1e-8, name=\"Adam\"):\n",
        "    # def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.699, use_locking=False, epsilon=1e-8, name=\"Adam\"):\n",
        "        super(AdamOptimizer, self).__init__(use_locking, name)\n",
        "        self._lr = learning_rate\n",
        "        self._beta1 = beta1\n",
        "        self._beta2 = beta2\n",
        "\n",
        "        # Tensor versions of the constructor arguments, created in _prepare().\n",
        "        self._lr_t = None\n",
        "        self._beta1_t = None\n",
        "        self._beta2_t = None\n",
        "        self._beta1_power = None\n",
        "        self._beta2_power = None\n",
        "\n",
        "    def _prepare(self):\n",
        "        self._lr_t = ops.convert_to_tensor(self._lr, name=\"learning_rate\")\n",
        "        self._beta1_t = ops.convert_to_tensor(self._beta1, name=\"beta1\")\n",
        "        self._beta2_t = ops.convert_to_tensor(self._beta2, name=\"beta2\")\n",
        "\n",
        "    def _create_slots(self, var_list):\n",
        "        # Create slots for the first and second moments.\n",
        "        first_var = min(var_list, key=lambda x: x.name)\n",
        "        with ops.colocate_with(first_var):\n",
        "            self._beta1_power = variable_scope.variable(self._beta1, name=\"beta1_power\", trainable=False)\n",
        "            self._beta2_power = variable_scope.variable(self._beta2, name=\"beta2_power\", trainable=False)\n",
        "        # Create slots for the first and second moments.\n",
        "        for v in var_list:\n",
        "            self._zeros_slot(v, \"m1\", self._name)\n",
        "            self._zeros_slot(v, \"v1\", self._name)\n",
        "\n",
        "    def _apply_dense(self, grad, var):\n",
        "        lr_t = math_ops.cast(self._lr_t, var.dtype.base_dtype)\n",
        "        beta1_power = math_ops.cast(self._beta1_power, var.dtype.base_dtype)\n",
        "        beta2_power = math_ops.cast(self._beta2_power, var.dtype.base_dtype)\n",
        "        beta1_t = math_ops.cast(self._beta1_t, var.dtype.base_dtype)\n",
        "        beta2_t = math_ops.cast(self._beta2_t, var.dtype.base_dtype)\n",
        "        eps = 1e-8 \n",
        "        v = self.get_slot(var, \"v1\")\n",
        "        v_t = v.assign(beta2_t * v + (1. - beta2_t) * grad**2)\n",
        "        m = self.get_slot(var, \"m1\")\n",
        "        m_t = m.assign(beta1_t * m + (1. - beta1_t) * grad)\n",
        "        alpha_t =  tf.sqrt(1 - beta2_power) / (1 - beta1_power) \n",
        "        g_t =  (m_t*alpha_t) / (tf.sqrt(v_t) + eps)\n",
        "        var_update = state_ops.assign_sub(var, lr_t * g_t)\n",
        "        return control_flow_ops.group(*[var_update, v_t, m_t])\n",
        "\n",
        "    def _apply_sparse(self, grad, var):\n",
        "        raise NotImplementedError(\"Sparse gradient updates are not supported.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZCM-Ul3lBYg"
      },
      "outputs": [],
      "source": [
        "optimizer_options = ['SGDNesterov', 'Adagrad', 'RMSProp', 'AdaDelta', 'Adam']\n",
        "dropout_options = [False]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT_Yr1C0okzW",
        "outputId": "f1738f8e-0ef8-4ee7-9cde-d8365409e369"
      },
      "outputs": [],
      "source": [
        "def optimizer_fn(optimizer, lr, loss, name='Optimizer'):\n",
        "    with tf.variable_scope(name):\n",
        "        global_step = tf.Variable(1, dtype=tf.float32, trainable=False)\n",
        "        cur_lr = lr / tf.math.sqrt(x=global_step)\n",
        "\n",
        "        if optimizer == 'SGDNesterov':\n",
        "            return tf.train.MomentumOptimizer(learning_rate=cur_lr,\n",
        "                                              momentum=0.99,\n",
        "                                              name='SGDNesterov',\n",
        "                                              use_nesterov=True).minimize(loss, global_step=global_step), cur_lr\n",
        "        elif optimizer == 'Adagrad':\n",
        "            return tf.train.AdagradOptimizer(learning_rate=cur_lr).minimize(loss, global_step=global_step), cur_lr\n",
        "        elif optimizer == 'RMSProp':\n",
        "            return tf.train.RMSPropOptimizer(learning_rate=cur_lr).minimize(loss, global_step=global_step), cur_lr\n",
        "        elif optimizer == 'AdaDelta':\n",
        "            return tf.train.AdadeltaOptimizer(learning_rate=cur_lr).minimize(loss, global_step=global_step), cur_lr\n",
        "        elif optimizer == 'Adam':\n",
        "            return AdamOptimizer(learning_rate=cur_lr).minimize(loss, global_step=global_step), cur_lr\n",
        "        else:\n",
        "            raise NotImplementedError(\" [*] Optimizer is not included in list!\")\n",
        "\n",
        "learning_rate = 0.01\n",
        "epochs = 20\n",
        "batch_size = 100\n",
        "batches = int(x_train.shape[0] / batch_size)\n",
        "for optimizer in optimizer_options:\n",
        "  print('\\nOptimizer: {}\\n'.format(optimizer))\n",
        "  X = tf.placeholder(tf.float32, [None, 784])\n",
        "  Y = tf.placeholder(tf.float32, [None, 10])\n",
        "  W = tf.Variable(0.1 * np.random.randn(784, 10).astype(np.float32))\n",
        "  B = tf.Variable(0.1 * np.random.randn(10).astype(np.float32))\n",
        "  Y_predicted = tf.nn.softmax(tf.add(tf.matmul(X, W), B))\n",
        "  loss = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(Y_predicted), axis=1))\n",
        "  optimizer = optimizer_fn(optimizer,learning_rate,loss)\n",
        "  with tf.Session () as sess:\n",
        "    sess.run ( tf.global_variables_initializer ( ) )\n",
        "    for epoch in range(epochs):\n",
        "      for i in range(batches):\n",
        "        offset = i * epoch\n",
        "        x = x_train[offset: offset + batch_size]\n",
        "        y = y_train[offset: offset + batch_size]\n",
        "        sess.run(optimizer, feed_dict={X: x, Y:y})\n",
        "        c = sess.run(loss, feed_dict={X:x, Y:y})\n",
        "      print(f'epoch:{epoch:2d} cost={c:.4f}')\n",
        "    correct_pred = tf.equal(tf.argmax(Y_predicted, 1), tf.argmax(Y, 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "    acc = accuracy.eval({X: x_test, Y: y_test})\n",
        "    print(f'Accuracy: {acc * 100:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
