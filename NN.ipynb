{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q11m2EVupsUO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-F8Sm_rtAtH"
      },
      "outputs": [],
      "source": [
        "data = keras.datasets.fashion_mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkf8kk8Kt1pu",
        "outputId": "27dfe39a-73a4-46ff-ff91-533a1807b8c6"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = data.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKjuEMeJt-Pm"
      },
      "outputs": [],
      "source": [
        "class_names = ['t_shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
        "          'sandal', 'shirt', 'sneaker', 'bag', 'ankle_boots']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKGBhJhpuxiQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.ops import control_flow_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import state_ops\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.training import optimizer\n",
        "#from tensorflow.python.eager import context\n",
        "from tensorflow.python.ops import resource_variable_ops\n",
        "from tensorflow.python.ops import variable_scope\n",
        "from tensorflow.python.training import training_ops\n",
        "\n",
        "from keras import backend_config\n",
        "from keras.optimizers.optimizer_v2 import optimizer_v2\n",
        "\n",
        "class AdamOptimizer(optimizer_v2.OptimizerV2):\n",
        "    def __init__(\n",
        "        self,\n",
        "        learning_rate=0.001,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-7,\n",
        "        amsgrad=False,\n",
        "        name=\"Adam\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(name, **kwargs)\n",
        "        self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate))\n",
        "        self._set_hyper(\"decay\", self._initial_decay)\n",
        "        self._set_hyper(\"beta_1\", beta_1)\n",
        "        self._set_hyper(\"beta_2\", beta_2)\n",
        "        self.epsilon = epsilon or backend_config.epsilon()\n",
        "        self.amsgrad = amsgrad\n",
        "\n",
        "    def _create_slots(self, var_list):\n",
        "        # Create slots for the first and second moments.\n",
        "        # Separate for-loops to respect the ordering of slot variables from v1.\n",
        "        for var in var_list:\n",
        "            self.add_slot(var, \"m\")\n",
        "        for var in var_list:\n",
        "            self.add_slot(var, \"v\")\n",
        "        if self.amsgrad:\n",
        "            for var in var_list:\n",
        "                self.add_slot(var, \"vhat\")\n",
        "\n",
        "    def _prepare_local(self, var_device, var_dtype, apply_state):\n",
        "        super()._prepare_local(var_device, var_dtype, apply_state)\n",
        "\n",
        "        local_step = tf.cast(self.iterations + 1, var_dtype)\n",
        "        beta_1_t = tf.identity(self._get_hyper(\"beta_1\", var_dtype))\n",
        "        beta_2_t = tf.identity(self._get_hyper(\"beta_2\", var_dtype))\n",
        "        beta_1_power = tf.pow(beta_1_t, local_step)\n",
        "        beta_2_power = tf.pow(beta_2_t, local_step)\n",
        "        lr = apply_state[(var_device, var_dtype)][\"lr_t\"] * (\n",
        "            tf.sqrt(1 - beta_2_power) / (1 - beta_1_power)\n",
        "        )\n",
        "        apply_state[(var_device, var_dtype)].update(\n",
        "            dict(\n",
        "                lr=lr,\n",
        "                epsilon=tf.convert_to_tensor(self.epsilon, var_dtype),\n",
        "                beta_1_t=beta_1_t,\n",
        "                beta_1_power=beta_1_power,\n",
        "                one_minus_beta_1_t=1 - beta_1_t,\n",
        "                beta_2_t=beta_2_t,\n",
        "                beta_2_power=beta_2_power,\n",
        "                one_minus_beta_2_t=1 - beta_2_t,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        params = self.weights\n",
        "        # If the weights are generated by Keras V1 optimizer, it includes vhats\n",
        "        # even without amsgrad, i.e, V1 optimizer has 3x + 1 variables, while V2\n",
        "        # optimizer has 2x + 1 variables. Filter vhats out for compatibility.\n",
        "        num_vars = int((len(params) - 1) / 2)\n",
        "        if len(weights) == 3 * num_vars + 1:\n",
        "            weights = weights[: len(params)]\n",
        "        super().set_weights(weights)\n",
        "\n",
        "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
        "        var_device, var_dtype = var.device, var.dtype.base_dtype\n",
        "        coefficients = (apply_state or {}).get(\n",
        "            (var_device, var_dtype)\n",
        "        ) or self._fallback_apply_state(var_device, var_dtype)\n",
        "\n",
        "        m = self.get_slot(var, \"m\")\n",
        "        v = self.get_slot(var, \"v\")\n",
        "\n",
        "        vhat = self.get_slot(var, \"vhat\")\n",
        "        return tf.raw_ops.ResourceApplyAdamWithAmsgrad(\n",
        "            var=var.handle,\n",
        "            m=m.handle,\n",
        "            v=v.handle,\n",
        "            vhat=vhat.handle,\n",
        "            beta1_power=coefficients[\"beta_1_power\"],\n",
        "            beta2_power=coefficients[\"beta_2_power\"],\n",
        "            lr=coefficients[\"lr_t\"],\n",
        "            beta1=coefficients[\"beta_1_t\"],\n",
        "            beta2=coefficients[\"beta_2_t\"],\n",
        "            epsilon=coefficients[\"epsilon\"],\n",
        "            grad=grad,\n",
        "            use_locking=self._use_locking,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7ibdT68uyce"
      },
      "outputs": [],
      "source": [
        "def optimizer_fn(optimizer, lr, name='Optimizer'):\n",
        "    with tf.compat.v1.variable_scope(name):\n",
        "        global_step = tf.Variable(1, dtype=tf.float32, trainable=False)\n",
        "        cur_lr = lr / tf.math.sqrt(x=global_step)\n",
        "\n",
        "        if optimizer == 'SGDNesterov':\n",
        "            return tf.keras.optimizers.SGD(learning_rate=lr,momentum=0.99,nesterov=True)\n",
        "        elif optimizer == 'Adagrad':\n",
        "            return tf.keras.optimizers.Adagrad(learning_rate=cur_lr)\n",
        "        elif optimizer == 'RMSProp':\n",
        "            return tf.keras.optimizers.RMSprop(learning_rate=cur_lr)\n",
        "        elif optimizer == 'AdaDelta':\n",
        "            return tf.keras.optimizers.Adadelta(learning_rate=cur_lr)\n",
        "        elif optimizer == 'Adam':\n",
        "            return AdamOptimizer(learning_rate=cur_lr)\n",
        "        else:\n",
        "            raise NotImplementedError(\" [*] Optimizer is not included in list!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrzEAi7uu0ou"
      },
      "outputs": [],
      "source": [
        "train_images = train_images/255.0\n",
        "test_images = test_images/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def getModel(dropout,rate=0.2):\n",
        "    if dropout:\n",
        "        model = keras.Sequential([\n",
        "\t\tkeras.layers.Flatten(input_shape=(28,28)),\n",
        "\t\tkeras.layers.Dropout(rate),\n",
        "\t\tkeras.layers.Dense(1000, activation=\"relu\"),\n",
        "\t\tkeras.layers.Dropout(rate),\n",
        "\t\tkeras.layers.Dense(1000, activation=\"relu\"),\n",
        "\t\tkeras.layers.Dropout(rate),\n",
        "\t\tkeras.layers.Dense(10, activation=\"softmax\")\n",
        "\t\t])\n",
        "        return model\n",
        "    else:\n",
        "        model = keras.Sequential([\n",
        "\t\tkeras.layers.Flatten(input_shape=(28,28)),\n",
        "\t\tkeras.layers.Dense(1000, activation=\"relu\"),\n",
        "\t\tkeras.layers.Dense(1000, activation=\"relu\"),\n",
        "\t\tkeras.layers.Dense(10, activation=\"softmax\")\n",
        "\t\t])\n",
        "        return model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def NN(lr):\n",
        "\toptimizer_list=['SGDNesterov','Adagrad','RMSProp','AdaDelta','Adam']\n",
        "\tdropout_list=[True,False]\n",
        "\tfor optimizer in optimizer_list:\n",
        "\t\tfor dropout in dropout_list:\n",
        "\t\t\tprint(\"Optimizer: \",optimizer)\n",
        "\t\t\tprint(\"Dropout: \",dropout)\n",
        "\t\t\tmodel = getModel(dropout)\n",
        "\t\t\tmodel.compile(optimizer=optimizer_fn(optimizer, lr), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\t\t\tmodel.fit(train_images, train_labels, epochs=20)\n",
        "\t\t\ttest_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "\t\t\tprint('Test accuracy:', test_acc)\n",
        "\t\t\tprint(\"===========================================\")\n",
        "\n",
        "\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NN(0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NN(0.01)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
