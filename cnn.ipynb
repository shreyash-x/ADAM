{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UR-KsWaiFmpn",
        "outputId": "6670ad71-d943-43d6-e611-5fc6ca81030d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cifar10 = keras.datasets.cifar10\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "print(train_images.shape) # 50000, 32, 32, 3\n",
        "\n",
        "# Normalize: 0,255 -> 0,1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "def show():\n",
        "    plt.figure(figsize=(10,10))\n",
        "    for i in range(16):\n",
        "        plt.subplot(4,4,i+1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "        # The CIFAR labels happen to be arrays, \n",
        "        # which is why you need the extra index\n",
        "        plt.xlabel(class_names[train_labels[i][0]])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_options = ['SGDNesterov', 'Adagrad', 'RMSProp', 'AdaDelta', 'Adam']\n",
        "learning_rate = [0.01,0.001]\n",
        "dropout_options = [False,True]"
      ],
      "metadata": {
        "id": "EI6QJvKKceol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def optimizer_fn(optimizer, lr, name='Optimizer'):\n",
        "    with tf.compat.v1.variable_scope(name):\n",
        "        global_step = tf.Variable(1, dtype=tf.float32, trainable=False)\n",
        "        cur_lr = lr / tf.math.sqrt(x=global_step)\n",
        "\n",
        "        if optimizer == 'SGDNesterov':\n",
        "            return tf.keras.optimizers.SGD(learning_rate=lr,momentum=0.99,nesterov=True)\n",
        "        elif optimizer == 'Adagrad':\n",
        "            return tf.keras.optimizers.Adagrad(learning_rate=cur_lr)\n",
        "        elif optimizer == 'RMSProp':\n",
        "            return tf.keras.optimizers.RMSprop(learning_rate=cur_lr)\n",
        "        elif optimizer == 'AdaDelta':\n",
        "            return tf.keras.optimizers.Adadelta(learning_rate=cur_lr)\n",
        "        elif optimizer == 'Adam':\n",
        "            return AdamOptimizer(learning_rate=cur_lr)\n",
        "        else:\n",
        "            raise NotImplementedError(\" [*] Optimizer is not included in list!\")"
      ],
      "metadata": {
        "id": "cdVrQW6gc5vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops import control_flow_ops\n",
        "from tensorflow.python.ops import math_ops\n",
        "from tensorflow.python.ops import state_ops\n",
        "from tensorflow.python.framework import ops\n",
        "from tensorflow.python.training import optimizer\n",
        "#from tensorflow.python.eager import context\n",
        "from tensorflow.python.ops import resource_variable_ops\n",
        "from tensorflow.python.ops import variable_scope\n",
        "from tensorflow.python.training import training_ops\n",
        "\n",
        "from keras import backend_config\n",
        "from keras.optimizers.optimizer_v2 import optimizer_v2\n",
        "\n",
        "class AdamOptimizer(optimizer_v2.OptimizerV2):\n",
        "    def __init__(\n",
        "        self,\n",
        "        learning_rate=0.001,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-7,\n",
        "        amsgrad=False,\n",
        "        name=\"Adam\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(name, **kwargs)\n",
        "        self._set_hyper(\"learning_rate\", kwargs.get(\"lr\", learning_rate))\n",
        "        self._set_hyper(\"decay\", self._initial_decay)\n",
        "        self._set_hyper(\"beta_1\", beta_1)\n",
        "        self._set_hyper(\"beta_2\", beta_2)\n",
        "        self.epsilon = epsilon or backend_config.epsilon()\n",
        "        self.amsgrad = amsgrad\n",
        "\n",
        "    def _create_slots(self, var_list):\n",
        "        for var in var_list:\n",
        "            self.add_slot(var, \"m\")\n",
        "        for var in var_list:\n",
        "            self.add_slot(var, \"v\")\n",
        "        if self.amsgrad:\n",
        "            for var in var_list:\n",
        "                self.add_slot(var, \"vhat\")\n",
        "\n",
        "    def _prepare_local(self, var_device, var_dtype, apply_state):\n",
        "        super()._prepare_local(var_device, var_dtype, apply_state)\n",
        "\n",
        "        local_step = tf.cast(self.iterations + 1, var_dtype)\n",
        "        beta_1_t = tf.identity(self._get_hyper(\"beta_1\", var_dtype))\n",
        "        beta_2_t = tf.identity(self._get_hyper(\"beta_2\", var_dtype))\n",
        "        beta_1_power = tf.pow(beta_1_t, local_step)\n",
        "        beta_2_power = tf.pow(beta_2_t, local_step)\n",
        "        stepSizeUpperBoundParameter = tf.sqrt(1 - beta_2_power) / (1 - beta_1_power)\n",
        "        lr = apply_state[(var_device, var_dtype)][\"lr_t\"] * (\n",
        "            stepSizeUpperBoundParameter\n",
        "        )\n",
        "        apply_state[(var_device, var_dtype)].update(\n",
        "            dict(\n",
        "                lr=lr,\n",
        "                epsilon=tf.convert_to_tensor(self.epsilon, var_dtype),\n",
        "                beta_1_t=beta_1_t,\n",
        "                beta_1_power=beta_1_power,\n",
        "                one_minus_beta_1_t=1 - beta_1_t,\n",
        "                beta_2_t=beta_2_t,\n",
        "                beta_2_power=beta_2_power,\n",
        "                one_minus_beta_2_t=1 - beta_2_t,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        params = self.weights\n",
        "        num_vars = int((len(params) - 1) / 2)\n",
        "        total_vars = 3 * num_vars + 1\n",
        "        newWeights = []\n",
        "        if len(weights) == total_vars:\n",
        "            for i in range(0,len(params)):\n",
        "              newWeights.append(weights[i])\n",
        "        super().set_weights(newWeights)\n",
        "\n",
        "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
        "        var_device, var_dtype = var.device, var.dtype.base_dtype\n",
        "        coefficients = (apply_state or {}).get(\n",
        "            (var_device, var_dtype)\n",
        "        ) or self._fallback_apply_state(var_device, var_dtype)\n",
        "\n",
        "        m = self.get_slot(var, \"m\")\n",
        "        v = self.get_slot(var, \"v\")\n",
        "\n",
        "        if self.amsgrad:\n",
        "            vhat = self.get_slot(var, \"vhat\")\n",
        "            return tf.raw_ops.ResourceApplyAdamWithAmsgrad(\n",
        "                var=var.handle,\n",
        "                m=m.handle,\n",
        "                v=v.handle,\n",
        "                vhat=vhat.handle,\n",
        "                beta1_power=coefficients[\"beta_1_power\"],\n",
        "                beta2_power=coefficients[\"beta_2_power\"],\n",
        "                lr=coefficients[\"lr_t\"],\n",
        "                beta1=coefficients[\"beta_1_t\"],\n",
        "                beta2=coefficients[\"beta_2_t\"],\n",
        "                epsilon=coefficients[\"epsilon\"],\n",
        "                grad=grad,\n",
        "                use_locking=self._use_locking,\n",
        "            )\n",
        "        else:\n",
        "            return tf.raw_ops.ResourceApplyAdam(\n",
        "                var=var.handle,\n",
        "                m=m.handle,\n",
        "                v=v.handle,\n",
        "                beta1_power=coefficients[\"beta_1_power\"],\n",
        "                beta2_power=coefficients[\"beta_2_power\"],\n",
        "                lr=coefficients[\"lr_t\"],\n",
        "                beta1=coefficients[\"beta_1_t\"],\n",
        "                beta2=coefficients[\"beta_2_t\"],\n",
        "                epsilon=coefficients[\"epsilon\"],\n",
        "                grad=grad,\n",
        "                use_locking=self._use_locking,\n",
        "            )"
      ],
      "metadata": {
        "id": "5B4uCpWmcvrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def running_model(optimizer,dropout,lr):\n",
        "  model = keras.models.Sequential()\n",
        "  for i in range(3):\n",
        "    model.add(layers.Conv2D(32, (5,5), strides=(1,1), padding=\"valid\", activation='relu', input_shape=(32,32,3)))\n",
        "    model.add(layers.MaxPool2D(pool_size=(3, 3),strides=(2,2), padding=\"same\"))\n",
        "  model.add(layers.Flatten())\n",
        "  if(dropout):\n",
        "    model.add(layers.Dropout(0.2))\n",
        "  model.add(layers.Dense(1000, activation='relu'))\n",
        "  model.add(layers.Dense(10))\n",
        "  print(model.summary())\n",
        "  #import sys; sys.exit()\n",
        "\n",
        "  # loss and optimizer\n",
        "  loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "  optim = optimizer_fn(optimizer,lr)\n",
        "  metrics = [\"accuracy\"]\n",
        "\n",
        "  model.compile(optimizer=optim, loss=loss, metrics=metrics)\n",
        "\n",
        "  # training\n",
        "  batch_size = 128\n",
        "  epochs = 20\n",
        "\n",
        "  model.fit(train_images, train_labels, epochs=epochs,\n",
        "            batch_size=batch_size, verbose=2)\n",
        "\n",
        "  # evaulate\n",
        "  model.evaluate(test_images,  test_labels, batch_size=batch_size, verbose=2)"
      ],
      "metadata": {
        "id": "Ylzh9teVcngK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for lr in learning_rate:\n",
        "  for optimizer in optimizer_options:\n",
        "    for dropout in dropout_options:\n",
        "      print('\\nOptimizer: {}\\tDropout option: {}\\t Learning Rate: {}\\n'.format(optimizer, dropout,lr))\n",
        "      running_model(optimizer,dropout,lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga57cNtjdbHM",
        "outputId": "03ddb82c-a609-4459-fe76-ab3ff9796543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimizer: Adam\tDropout option: False\t Learning Rate: 0.001\n",
            "\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_12 (Conv2D)          (None, 28, 28, 32)        2432      \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPoolin  (None, 14, 14, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 10, 10, 32)        25632     \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPoolin  (None, 5, 5, 32)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 1, 1, 32)          25632     \n",
            "                                                                 \n",
            " max_pooling2d_14 (MaxPoolin  (None, 1, 1, 32)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1000)              33000     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 10)                10010     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 96,706\n",
            "Trainable params: 96,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/2\n",
            "391/391 - 89s - loss: 1.6613 - accuracy: 0.3853 - 89s/epoch - 227ms/step\n",
            "Epoch 2/2\n",
            "391/391 - 77s - loss: 1.3115 - accuracy: 0.5257 - 77s/epoch - 196ms/step\n",
            "79/79 - 4s - loss: 1.2858 - accuracy: 0.5339 - 4s/epoch - 46ms/step\n",
            "\n",
            "Optimizer: Adam\tDropout option: True\t Learning Rate: 0.001\n",
            "\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_15 (Conv2D)          (None, 28, 28, 32)        2432      \n",
            "                                                                 \n",
            " max_pooling2d_15 (MaxPoolin  (None, 14, 14, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_16 (Conv2D)          (None, 10, 10, 32)        25632     \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPoolin  (None, 5, 5, 32)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_17 (Conv2D)          (None, 1, 1, 32)          25632     \n",
            "                                                                 \n",
            " max_pooling2d_17 (MaxPoolin  (None, 1, 1, 32)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 32)                0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1000)              33000     \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 10)                10010     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 96,706\n",
            "Trainable params: 96,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/2\n",
            "391/391 - 75s - loss: 1.7441 - accuracy: 0.3374 - 75s/epoch - 193ms/step\n",
            "Epoch 2/2\n",
            "391/391 - 76s - loss: 1.4611 - accuracy: 0.4594 - 76s/epoch - 194ms/step\n",
            "79/79 - 4s - loss: 1.2911 - accuracy: 0.5277 - 4s/epoch - 45ms/step\n",
            "\n",
            "Optimizer: Adam\tDropout option: False\t Learning Rate: 0.01\n",
            "\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_18 (Conv2D)          (None, 28, 28, 32)        2432      \n",
            "                                                                 \n",
            " max_pooling2d_18 (MaxPoolin  (None, 14, 14, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 10, 10, 32)        25632     \n",
            "                                                                 \n",
            " max_pooling2d_19 (MaxPoolin  (None, 5, 5, 32)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 1, 1, 32)          25632     \n",
            "                                                                 \n",
            " max_pooling2d_20 (MaxPoolin  (None, 1, 1, 32)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 1000)              33000     \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 10)                10010     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 96,706\n",
            "Trainable params: 96,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/2\n",
            "391/391 - 72s - loss: 2.0268 - accuracy: 0.2044 - 72s/epoch - 183ms/step\n",
            "Epoch 2/2\n",
            "391/391 - 70s - loss: 1.8601 - accuracy: 0.2928 - 70s/epoch - 180ms/step\n",
            "79/79 - 4s - loss: 1.8249 - accuracy: 0.3090 - 4s/epoch - 46ms/step\n",
            "\n",
            "Optimizer: Adam\tDropout option: True\t Learning Rate: 0.01\n",
            "\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_21 (Conv2D)          (None, 28, 28, 32)        2432      \n",
            "                                                                 \n",
            " max_pooling2d_21 (MaxPoolin  (None, 14, 14, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_22 (Conv2D)          (None, 10, 10, 32)        25632     \n",
            "                                                                 \n",
            " max_pooling2d_22 (MaxPoolin  (None, 5, 5, 32)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_23 (Conv2D)          (None, 1, 1, 32)          25632     \n",
            "                                                                 \n",
            " max_pooling2d_23 (MaxPoolin  (None, 1, 1, 32)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 32)                0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1000)              33000     \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 10)                10010     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 96,706\n",
            "Trainable params: 96,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/2\n",
            "391/391 - 70s - loss: 2.3096 - accuracy: 0.1003 - 70s/epoch - 179ms/step\n",
            "Epoch 2/2\n",
            "391/391 - 72s - loss: 2.3034 - accuracy: 0.0966 - 72s/epoch - 185ms/step\n",
            "79/79 - 4s - loss: 2.3029 - accuracy: 0.1000 - 4s/epoch - 46ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_loss(file_name,title):\n",
        "    with open(file_name) as csv_file:\n",
        "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "        for row in csv_reader:\n",
        "            optimizer_name = row[0]\n",
        "            loss_values = row[1:]\n",
        "            loss_values = [float(i) for i in loss_values]\n",
        "            x_axis = []\n",
        "            for i in range(len(loss_values)):\n",
        "                x_axis.append(i+1)\n",
        "            plt.plot(x_axis,loss_values,label=optimizer_name)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.ylabel(\"loss\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "2cOLGDS0bfkT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}